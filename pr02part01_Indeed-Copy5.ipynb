{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multiple output on Jupyter notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# to open the web page for web scraping\n",
    "import requests\n",
    "\n",
    "# to parse html \n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "# selenium or splinter for \n",
    "from splinter import Browser\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_URL(job,city):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    base_url = 'http://www.indeed.com/jobs?q='\n",
    "    add_1_job= job\n",
    "    add_precity = '&l='\n",
    "    add_2_city=city\n",
    "    return (base_url + add_1_job + add_precity + add_2_city)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(compile_URL('developer','denver'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bring_soup(url):\n",
    "    html_source= requests.get(url)\n",
    "    soup = BeautifulSoup(html_source.content, 'html.parser', from_encoding=\"utf-8\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bring_soup(compile_URL('developer','denver'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_column(soup):\n",
    "    col_title =[]\n",
    "    for itersoup in soup.find_all(class_= \"result\" ):\n",
    "        # - - - - \n",
    "        try:\n",
    "            itertitle = itersoup.find(class_='jobtitle').\\\n",
    "            text.replace('\\n', '')\n",
    "        except:\n",
    "            itertitle='None'\n",
    "        # - - - - \n",
    "        col_title.append(itertitle)\n",
    "        \n",
    "    return col_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_column(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_column(soup):\n",
    "    col_location  = [ ]\n",
    "    for itersoup in soup.find_all(class_= \"result\" ):\n",
    "            try:\n",
    "                location = itersoup.find('span', {'class':\"location\" }).\\\n",
    "                text.replace('\\n', '')\n",
    "            except:\n",
    "                location = 'None'\n",
    "#             - - -\n",
    "            col_location.append(location)\n",
    "    return col_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location_column(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def company_column(soup):\n",
    "    col_company  = [ ]\n",
    "    for itersoup in soup.find_all(class_= \"result\" ):\n",
    "            try:\n",
    "                company = itersoup.find(class_='company').\\\n",
    "                text.replace('\\n','')\n",
    "            except:\n",
    "                company = 'None'\n",
    "#             - - -\n",
    "            col_company.append(company.strip())\n",
    "    return col_company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company_column(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_column(soup):\n",
    "    col_desc  = [ ]\n",
    "    for itersoup in soup.find_all(class_= \"result\" ):\n",
    "            try:\n",
    "                description = itersoup.find('span', {'class':'summary'}).\\\n",
    "                text.replace('\\n', '')\n",
    "            except:\n",
    "                description = 'None'\n",
    "#             - - -\n",
    "            col_desc.append(description.strip())\n",
    "    return col_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desc_column(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_column(soup):\n",
    "    col_href = []\n",
    "    base_url = 'http://www.indeed.com'\n",
    "    for iterfoo in soup.find_all(class_= \"result\" ):\n",
    "        try:\n",
    "            href = iterfoo.find('a')['href']\n",
    "            col_href.append((base_url+href).strip())\n",
    "        except:\n",
    "            href='None'\n",
    "            col_href.append(href)\n",
    "        \n",
    "    return list(col_href)\n",
    "    #     print('http://www.indeed.com'+href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_indeed(soup):\n",
    "    \"\"\" this function still can not make soup \"\"\"\n",
    "    return pd.DataFrame({'Title': title_column(soup),\n",
    "              'Company': company_column(soup),\n",
    "              'Location': location_column(soup),\n",
    "              'Description': desc_column(soup),\n",
    "              'URLs': url_column(soup)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the main function of our app's JobLister module\n",
    "# ====================================\n",
    "# compile_URL('developer','denver')\n",
    "def parse_n_frame(url):\n",
    "#     /*  this function brings soup herself  */\n",
    "    soup = bring_soup(url)\n",
    "    return pd.DataFrame({'Title': title_column(soup),\n",
    "              'Company': company_column(soup),\n",
    "              'Location': location_column(soup),\n",
    "              'Description': desc_column(soup),\n",
    "              'URLs': url_column(soup)})\n",
    "#============end==============\n",
    "\n",
    "#===========test==============\n",
    "# parse_n_frame(dev_denver_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_browser():\n",
    "    path_chromedriver = \"C:/Users/DENVER/Documents/0000_Attila_Codes_Apps_Hw_Projects/DU_BigData_pr2_ETL_MySQL_Python/chromedriver\"\n",
    "    executable_path = {\n",
    "        \"executable_path\":\n",
    "        \"C:/Users/DENVER/Documents/0000_Attila_Codes_Apps_Hw_Projects/DU_BigData_pr2_ETL_MySQL_Python/chromedriver\"\n",
    "    }\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)\n",
    "#  - - -\n",
    "# ----test----\n",
    "browser = init_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_all_jobs(url):\n",
    "    browser = init_browser()\n",
    "    for i in range(1,(len(url_list)-10)):\n",
    "        browser.visit(url_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobListSample = ['data+analyst',\n",
    "            'developer',\n",
    "            'programmer',\n",
    "            'front+end+developer',\n",
    "            'back+end+developer',\n",
    "            'full+stack+developer'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityListSample = ['denver',\n",
    "                  'boulder',\n",
    "             'phoenix',\n",
    "             'houston',\n",
    "             'tucson',\n",
    "             'chicago',\n",
    "             'new+york',\n",
    "             'albuquerque',\n",
    "             'seattle',\n",
    "             'san+francisco'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapListFrame(job_list, city_list):\n",
    "    pass \n",
    "    list_of_all_frames =[]\n",
    "    for job in job_list:\n",
    "        for city in city_list:\n",
    "            __url__ = compile_URL(job,city)\n",
    "            __df__= parse_n_frame(__url__)\n",
    "            list_of_all_frames.append(__df__)            \n",
    "    return pd.concat(list_of_all_frames)\n",
    "# -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrapListFrame(jobListSample,cityListSample)\n",
    "# returns dataframe apprx 1,000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store above data consisting of concatanated dataframes\n",
    "concat_df=scrapListFrame(jobListSample,cityListSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(concat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# write scraped data to csv\n",
    "file_output_csv01 = str(time.strftime(\"%m-%d-%y\")) +str(np.random.randint(1,9+1)) + \"-jobs.csv\"\n",
    "concat_df.to_csv(\"csv/\" + file_output_csv01, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_dict = concat_df.to_dict('records')\n",
    "# converted a dataframe object to list of dicts for mongoDB\n",
    "# db.insert_many(concat_df.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas\n",
    "out = concat_df.to_json(orient='records')[1:-1].replace('},{', '} {')\n",
    "# type(out) returns STR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551835"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('file_name.json', 'w') as f:\n",
    "    f.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
